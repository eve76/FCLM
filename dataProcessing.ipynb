{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('flaky_data.xlsx', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Language', 'label', 'test case content'])\n",
    "df.rename(columns={'test case content': 'text'}, inplace=True)\n",
    "df['tokens'] = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language\n",
       "Java      1904\n",
       "Python     210\n",
       "go          90\n",
       "C++         78\n",
       "JS          61\n",
       "PHP          2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language\n",
       "Java      1904\n",
       "Python     210\n",
       "go          90\n",
       "C++         78\n",
       "JS          61\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df['Language'] == 'PHP'].index, inplace=True)\n",
    "df.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NonFlaky    1321\n",
       "Flaky       1022\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter the dataset according to languages\n",
    "\n",
    "java_df = df[df['Language'] == 'Java']\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "go_df = df[df['Language'] == 'go']\n",
    "cpp_df = df[df['Language'] == 'C++']\n",
    "js_df = df[df['Language'] == 'JS']\n",
    "java_js_df = df[df['Language'].isin(['Java', 'JS'])]\n",
    "java_py_cpp = df[df['Language'].isin(['Java', 'Python', 'C++'])]\n",
    "no_java = df[~df['Language'].isin(['Java'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(data, model):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    \n",
    "    # processed_idxs = []\n",
    "    predictions = []\n",
    "    yes_prob = []\n",
    "    gt = []\n",
    "    n_skipped_functions = 0\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        min_new_tokens=1,\n",
    "        max_new_tokens=3,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        \n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "        \n",
    "        # Decode the generated tokens\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the prediction from the decoded output\n",
    "        prediction = re.search(r'\\b(yes|no)\\b', decoded_output, re.IGNORECASE)\n",
    "        \n",
    "        if prediction:\n",
    "            pred_label = prediction.group(0).lower()\n",
    "            predictions.append(pred_label)\n",
    "            yes_prob.append(pred_label == 'yes')\n",
    "            gt.append(label)\n",
    "            \n",
    "  \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
